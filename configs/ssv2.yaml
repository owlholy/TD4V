resume:
pretrain:
seed: 1024
data:
    dataset: somethingv2
    modality: RGB
    num_segments: 8
    seg_length: 1
    batch_size: 16
    workers: 4
    num_classes: 174
    image_tmpl: '{:06d}.jpg'
    train_root:
    val_root:
    train_list:
    val_list:
    label_list:
    input_size: 224
    random_shift: True
    num_sample: 2
    rand_aug: True
    rand_erase: False
network:
    arch: ViT-B/16
    init: True
    tm: False # localuni t1d atm False
    drop_out: 0.0
    emb_dropout: 0.0 
    type: clip_sth
    sim_header: Transf  # Transf   None
    joint_st: False
    drop_fc: 0
    n_emb: 320
    side_dim: 320
    fix_clip: False
    my_fix_clip: True
    fine_tuning: false
    DA_ratio: 1/6
    t_ratio: 1
    sample_mode: "CNN"
    pyramid:
        n_embs: [ 96, 96, 192, 192, 384, 384, 384, 384, 384, 384, 768, 768 ]
        # [96, 96, 192, 192, 384, 384, 384, 384, 384, 384, 768, 768]  # mlp = 1
        # [64, 64, 128, 128, 256, 256, 256, 256, 256, 256, 512, 512]  # 3
        # [96, 96, 192, 192, 192, 384, 384, 384, 768, 768, 768, 768]  # mlp = 0
        # [96, 96, 192, 192, 384, 384, 384, 384, 384, 768, 768, 768]  # mlp = 1/4
        # [96, 96, 96, 96, 192, 192, 192, 384, 384, 384, 768, 768]  # mlp = 2
        # [96, 96, 96, 96, 192, 192, 192, 192, 384, 384, 384, 768]  # mlp = 4
        mlp_ratio: 1.0
solver:
    type: cosine
    epochs: 30
    start_epoch: 0
    epoch_offset: 0
    optim: adamw
    lr: 1.e-3  # 1.e-3
    lr_warmup_step: 4
    weight_decay: 0.15
    betas: [0.9, 0.999]
    loss_type: CE
    evaluate: False
    clip_ratio: 1
    grad_accumulation_steps: 1
    # mixup: True
    smoothing: 0.1
    layer_decay: 1.0 # 0.7
logging:
    print_freq: 20
    eval_freq: 1
    skip_epoch: [1,3,4,5,7,9]
